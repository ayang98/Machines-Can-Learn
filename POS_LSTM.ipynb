{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses the treebank corpus from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "print(tagged_sentences)\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below use the COMP 182 HW 6 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos_file(filename):\n",
    "    \"\"\"\n",
    "    Parses an input tagged text file.\n",
    "    Input:\n",
    "    filename --- the file to parse\n",
    "    Returns: \n",
    "    The file represented as a list of tuples, where each tuple \n",
    "    is of the form (word, POS-tag).\n",
    "    A list of unique words found in the file.\n",
    "    A list of unique POS tags found in the file.\n",
    "    \"\"\"\n",
    "    file_representation = []\n",
    "    unique_words = set()\n",
    "    unique_tags = set()\n",
    "    f = open(str(filename), \"r\")\n",
    "    for line in f:\n",
    "        if len(line) < 2 or len(line.split(\"/\")) != 2:\n",
    "            continue\n",
    "        word = line.split(\"/\")[0].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        tag = line.split(\"/\")[1].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        file_representation.append( (word, tag) )\n",
    "        unique_words.add(word)\n",
    "        unique_tags.add(tag)\n",
    "    f.close()\n",
    "    return file_representation, unique_words, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: The / DT\n",
      "Line 2: final / JJ\n",
      "Line 3: major / JJ\n",
      "Line 4: items / NNS\n",
      "Line 5: of / IN\n",
      "Line 6: New / NNP\n",
      "Line 7: Deal / NNP\n",
      "Line 8: legislation / NN\n",
      "Line 9: were / VBD\n",
      "Line 10: the / DT\n",
      "Line 11: creation / NN\n",
      "Line 12: of / IN\n",
      "Line 13: the / DT\n",
      "Line 14: United / NNP\n",
      "Line 15: States / NNPS\n",
      "Line 16: Housing / NNP\n",
      "Line 17: Authority / NNP\n",
      "Line 18: and / CC\n",
      "Line 19: Farm / NNP\n",
      "Line 20: Security / NNP\n",
      "Line 21: Administration / NNP\n",
      "Line 22: , / ,\n",
      "Line 23: both / DT\n",
      "Line 24: in / IN\n",
      "Line 25: 1937 / CD\n",
      "Line 26: , / ,\n",
      "Line 27: and / CC\n",
      "Line 28: the / DT\n",
      "Line 29: Fair / NNP\n",
      "Line 30: Labor / NNP\n",
      "Line 31: Standards / NNP\n",
      "Line 32: Act / NNP\n",
      "Line 33: of / IN\n",
      "Line 34: 1938 / CD\n",
      "Line 35: , / ,\n",
      "Line 36: which / WDT\n",
      "Line 37: set / VBP\n",
      "Line 38: maximum / NN\n",
      "Line 39: hours / NNS\n",
      "Line 40: and / CC\n",
      "Line 41: minimum / NN\n",
      "Line 42: wages / NNS\n",
      "Line 43: for / IN\n",
      "Line 44: most / JJS\n",
      "Line 45: categories / NNS\n",
      "Line 46: of / IN\n",
      "Line 47: workers / NNS\n",
      "Line 48: . / .\n",
      "Line 49: \n",
      "Line 49: The / DT\n",
      "Line 50: economic / JJ\n",
      "Line 51: downturn / NN\n",
      "Line 52: of / IN\n",
      "Line 53: 1937 / CD\n",
      "Line 54: -- / :\n",
      "Line 55: 38 / CD\n",
      "Line 56: , / ,\n",
      "Line 57: and / CC\n",
      "Line 58: the / DT\n",
      "Line 59: bitter / JJ\n",
      "Line 60: split / NN\n",
      "Line 61: between / IN\n",
      "Line 62: the / DT\n",
      "Line 63: AFL / NNP\n",
      "Line 64: and / CC\n",
      "Line 65: CIO / NNP\n",
      "Line 66: labor / NN\n",
      "Line 67: unions / NNS\n",
      "Line 68: led / VBD\n",
      "Line 69: to / TO\n",
      "Line 70: major / JJ\n",
      "Line 71: Republican / JJ\n",
      "Line 72: gains / NNS\n",
      "Line 73: in / IN\n",
      "Line 74: Congress / NNP\n",
      "Line 75: in / IN\n",
      "Line 76: 1938 / CD\n",
      "Line 77: . / .\n",
      "Line 78: \n",
      "Line 78: Conservative / NNP\n",
      "Line 79: Republicans / NNPS\n",
      "Line 80: and / CC\n",
      "Line 81: Democrats / NNPS\n",
      "Line 82: in / IN\n",
      "Line 83: Congress / NNP\n",
      "Line 84: joined / VBD\n",
      "Line 85: in / IN\n",
      "Line 86: the / DT\n",
      "Line 87: informal / JJ\n",
      "Line 88: Conservative / NNP\n",
      "Line 89: Coalition / NNP\n",
      "Line 90: . / .\n",
      "Line 91: \n",
      "Line 91: By / IN\n",
      "Line 92: 1942 / CD\n",
      "Line 93: -- / :\n",
      "Line 94: 43 / CD\n",
      "Line 95: they / PRP\n",
      "Line 96: shut / VBP\n",
      "Line 97: down / RP\n",
      "Line 98: relief / NN\n",
      "Line 99: programs / NNS\n",
      "[('The', 'DT'), ('economic', 'JJ'), ('downturn', 'NN'), ('of', 'IN'), ('1937', 'CD'), ('--', ':'), ('38', 'CD'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('bitter', 'JJ'), ('split', 'NN'), ('between', 'IN'), ('the', 'DT'), ('AFL', 'NNP'), ('and', 'CC'), ('CIO', 'NNP'), ('labor', 'NN'), ('unions', 'NNS'), ('led', 'VBD'), ('to', 'TO'), ('major', 'JJ'), ('Republican', 'JJ'), ('gains', 'NNS'), ('in', 'IN'), ('Congress', 'NNP'), ('in', 'IN'), ('1938', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def create_training_sentences(train_filepath):\n",
    "    training_sentences = []\n",
    "    with open(train_filepath) as fp:  \n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        sentence = []\n",
    "        while line:   \n",
    "            #if (cnt%1000 == 0):\n",
    "                #print (cnt)\n",
    "            word = line.split(\"/\")[0].replace(\" \", \"\").replace(\"\\n\", \"\").strip()\n",
    "            tag = line.split(\"/\")[1].replace(\" \", \"\").replace(\"\\n\", \"\").strip()\n",
    "            sentence.append((word, tag))\n",
    "            if cnt < 100:\n",
    "                print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "            line = fp.readline() #read next line \n",
    "            cnt += 1\n",
    "            if line.isspace() == True: #if the line you are currently at is empty, then you have completed a sentence\n",
    "                if cnt < 100:\n",
    "                    print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "                #print (str(cnt+1) + ' space ass!')\n",
    "                training_sentences.append(sentence) #append the sentence to the list of all training sentences\n",
    "                line = fp.readline() #go to the next line\n",
    "                #print (training_sentences[0])\n",
    "                sentence = []\n",
    "                \n",
    "    return training_sentences\n",
    "\n",
    "\n",
    "comp182_training = create_training_sentences('training.txt')\n",
    "\n",
    "print (comp182_training[1]) #print the second sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'economic' 'downturn' 'of' '1937' '--' '38' ',' 'and' 'the'\n",
      " 'bitter' 'split' 'between' 'the' 'AFL' 'and' 'CIO' 'labor' 'unions' 'led'\n",
      " 'to' 'major' 'Republican' 'gains' 'in' 'Congress' 'in' '1938' '.']\n",
      "['DT' 'JJ' 'NN' 'IN' 'CD' ':' 'CD' ',' 'CC' 'DT' 'JJ' 'NN' 'IN' 'DT' 'NNP'\n",
      " 'CC' 'NNP' 'NN' 'NNS' 'VBD' 'TO' 'JJ' 'JJ' 'NNS' 'IN' 'NNP' 'IN' 'CD' '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in comp182_training:        #tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "# Let's see how a sequence looks\n",
    " \n",
    "print(sentences[1])\n",
    "print(sentence_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "(\n",
    "    train_sentences, \n",
    "    test_sentences,\n",
    "    train_tags, \n",
    "    test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word vocabulary dictionary\n",
    "\n",
    "words = set([])\n",
    "for sentence in train_sentences:\n",
    "    for word in sentence:\n",
    "        words.add(word.lower())\n",
    "        \n",
    "#create tag vocabulary dictionary\n",
    "\n",
    "tags = set([])\n",
    "for sentence_tag in sentence_tags:\n",
    "    for tag in sentence_tag:\n",
    "        tags.add(tag)\n",
    "        \n",
    "\n",
    "word_vocab = {} #dictionary mapping unique words to a unique integer\n",
    "word_vocab['-PAD-'] = 0  # The special value used for padding\n",
    "word_vocab['-OOV-'] = 1  # The special value used for OOVs\n",
    "i = 2\n",
    "for word in list(words):\n",
    "    word_vocab[word] = i\n",
    "    i+=1  \n",
    "\n",
    "    \n",
    "tag_vocab = {} #dictionary mapping unique tags to a unique integer\n",
    "tag_vocab['-PAD-'] = 0\n",
    "i = 1\n",
    "for tag in list(tags):\n",
    "    tag_vocab[tag] = i\n",
    "    i+=1  \n",
    "\n",
    "#print (tag_vocab)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31629 17698 22211 40055 74526 69586 21173 36599 56692 75689 18910  9279\n",
      " 53261 55676  1790 21538 53261  6112 31925 41091  4637  1790 29783  9922\n",
      " 49349 18910 63919 21538 36750 76133 69916 37629 51359 32823 35763 63919\n",
      " 35763  3378  6112 15510 27971 61031  3799 65863 28947 46854 21173 39559\n",
      " 36484]\n",
      "[457 497 169 497 327 402  37 428  36 428  37 202 500 202 287 202 500 457\n",
      " 169 497 497 287  37 497 402  37 202 202 318 640 402 513 457 497 414 202\n",
      " 414  37 457 497 497 156 520 327 202 202  37 428 158]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        if w.lower() in word_vocab:\n",
    "            s_int.append(word_vocab[w.lower()])\n",
    "        else:\n",
    "            s_int.append(word_vocab['-OOV-'])\n",
    "    train_sentences_X.append(np.array(s_int))\n",
    "    \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        if w.lower() in word_vocab:\n",
    "            s_int.append(word_vocab[w.lower()])\n",
    "        else:\n",
    "            s_int.append(word_vocab['-OOV-'])\n",
    "    \n",
    "    test_sentences_X.append(np.array(s_int))\n",
    " \n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for t in s:\n",
    "        s_int.append(tag_vocab[t])\n",
    "    train_tags_y.append(np.array(s_int))\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for t in s:\n",
    "        s_int.append(tag_vocab[t])\n",
    "    test_tags_y.append(np.array(s_int))\n",
    " \n",
    " \n",
    "\n",
    "train_sentences_X = (np.asarray(train_sentences_X))\n",
    "test_sentences_X = (np.asarray(test_sentences_X))\n",
    "train_tags_y = (np.asarray(train_tags_y))\n",
    "test_tags_y = (np.asarray(test_tags_y))\n",
    "print (train_sentences_X[0])\n",
    "print (train_tags_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for element in train_sentences:\n",
    "    lengths.append(len(element))\n",
    "MAX_LENGTH = max(lengths)\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31629 17698 22211 ...     0     0     0]\n",
      "[25396 40485 21173 ...     0     0     0]\n",
      "[457 497 169 ...   0   0   0]\n",
      "[457 497  37 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#pad sequences with 0s until length = MAX_LENGTH\n",
    "import tensorflow\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1134)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1134, 24)          1845624   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1134, 128)         45568     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1134, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1134, 256)         263168    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1134, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 1134, 128)         164352    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1134, 650)         83850     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1134, 650)         0         \n",
      "=================================================================\n",
      "Total params: 2,402,562\n",
      "Trainable params: 2,402,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "\n",
    "def POS_LSTM():\n",
    "    \n",
    "    inputs = Input(shape = (MAX_LENGTH, ))\n",
    "    X = Embedding(len(word_vocab), 24)(inputs)  \n",
    "    X = Bidirectional(LSTM(64, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dropout(0.8))(X)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dropout(0.8))(X)\n",
    "    X = Bidirectional(LSTM(64, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dense(len(tag_vocab)))(X)\n",
    "    outputs = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    " \n",
    "model = POS_LSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.asarray(cat_sequences)\n",
    "\n",
    "#print (len(tag_vocab))\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag_vocab))\n",
    "#print (cat_train_tags_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/5\n",
      "2504/2504 [==============================] - 84s 34ms/step - loss: 1.6103 - acc: 0.8625 - val_loss: 0.6161 - val_acc: 0.9064\n",
      "Epoch 2/5\n",
      "2504/2504 [==============================] - 77s 31ms/step - loss: 0.5210 - acc: 0.9050 - val_loss: 0.3645 - val_acc: 0.9064\n",
      "Epoch 3/5\n",
      "2504/2504 [==============================] - 73s 29ms/step - loss: 0.3449 - acc: 0.9072 - val_loss: 0.3230 - val_acc: 0.9110\n",
      "Epoch 4/5\n",
      "2504/2504 [==============================] - 75s 30ms/step - loss: 0.3076 - acc: 0.9147 - val_loss: 0.3034 - val_acc: 0.9150\n",
      "Epoch 5/5\n",
      "2504/2504 [==============================] - 78s 31ms/step - loss: 0.2951 - acc: 0.9163 - val_loss: 0.2940 - val_acc: 0.9162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b3f210a58>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag_vocab)), batch_size=128, epochs= 5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 12s 15ms/step\n",
      "acc: 91.49642094189484\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag_vocab)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
