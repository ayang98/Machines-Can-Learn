{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses the treebank corpus from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "print(tagged_sentences)\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below use the COMP 182 HW 6 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos_file(filename):\n",
    "    \"\"\"\n",
    "    Parses an input tagged text file.\n",
    "    Input:\n",
    "    filename --- the file to parse\n",
    "    Returns: \n",
    "    The file represented as a list of tuples, where each tuple \n",
    "    is of the form (word, POS-tag).\n",
    "    A list of unique words found in the file.\n",
    "    A list of unique POS tags found in the file.\n",
    "    \"\"\"\n",
    "    file_representation = []\n",
    "    unique_words = set()\n",
    "    unique_tags = set()\n",
    "    f = open(str(filename), \"r\")\n",
    "    for line in f:\n",
    "        if len(line) < 2 or len(line.split(\"/\")) != 2:\n",
    "            continue\n",
    "        if line.isspace() == True:\n",
    "            print (\"ass\")\n",
    "        word = line.split(\"/\")[0].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        tag = line.split(\"/\")[1].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        file_representation.append( (word, tag) )\n",
    "        unique_words.add(word)\n",
    "        unique_tags.add(tag)\n",
    "    f.close()\n",
    "    return file_representation, unique_words, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 5865: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-f018945657ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_pos_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-152-cc59f675ad9b>\u001b[0m in \u001b[0;36mread_pos_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0munique_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 5865: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "training_data, unique_word, unique_tag = read_pos_file('training.txt')\n",
    "\n",
    "print (training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
      " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
      " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
      " '.']\n",
      "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
      " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "# Let's see how a sequence looks\n",
    " \n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "(\n",
    "    train_sentences, \n",
    "    test_sentences,\n",
    "    train_tags, \n",
    "    test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-PAD-': 0, 'VBP': 1, 'IN': 2, '-LRB-': 3, 'NNS': 4, 'NNP': 5, ':': 6, 'WP$': 7, '#': 8, 'JJS': 9, '``': 10, 'NNPS': 11, 'RBR': 12, \"''\": 13, 'VBG': 14, 'WRB': 15, 'PRP$': 16, 'VBN': 17, 'VBD': 18, 'WP': 19, '$': 20, 'VBZ': 21, 'WDT': 22, 'JJR': 23, 'SYM': 24, 'DT': 25, 'POS': 26, 'VB': 27, 'TO': 28, 'FW': 29, 'LS': 30, 'RB': 31, 'CC': 32, 'RBS': 33, 'EX': 34, '-NONE-': 35, 'PRP': 36, '.': 37, ',': 38, 'NN': 39, 'UH': 40, 'JJ': 41, 'CD': 42, 'RP': 43, 'MD': 44, 'PDT': 45, '-RRB-': 46}\n"
     ]
    }
   ],
   "source": [
    "#create word vocabulary dictionary\n",
    "\n",
    "words = set([])\n",
    "for sentence in train_sentences:\n",
    "    for word in sentence:\n",
    "        words.add(word.lower())\n",
    "        \n",
    "#create tag vocabulary dictionary\n",
    "\n",
    "tags = set([])\n",
    "for sentence_tag in sentence_tags:\n",
    "    for tag in sentence_tag:\n",
    "        tags.add(tag)\n",
    "        \n",
    "\n",
    "word_vocab = {} #dictionary mapping unique words to a unique integer\n",
    "word_vocab['-PAD-'] = 0  # The special value used for padding\n",
    "word_vocab['-OOV-'] = 1  # The special value used for OOVs\n",
    "i = 2\n",
    "for word in list(words):\n",
    "    word_vocab[word] = i\n",
    "    i+=1  \n",
    "\n",
    "    \n",
    "tag_vocab = {} #dictionary mapping unique tags to a unique integer\n",
    "tag_vocab['-PAD-'] = 0\n",
    "i = 1\n",
    "for tag in list(tags):\n",
    "    tag_vocab[tag] = i\n",
    "    i+=1  \n",
    "\n",
    "print (tag_vocab)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6193 4079  760 7038 1727 3884 6193 1727 9490 3352 5408  887 3884 7053\n",
      "  687 8522 4312 4465 8525 7856 4917 5102 1956 1819 3884 9561 4912 2499]\n",
      "[25 39 21  2 39 38 25 39  2 39  4 14 38 32 34 21 31 39 41 32 41  2 25 41\n",
      " 38 41  4 37]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        if w.lower() in word_vocab:\n",
    "            s_int.append(word_vocab[w.lower()])\n",
    "        else:\n",
    "            s_int.append(word_vocab['-OOV-'])\n",
    "    train_sentences_X.append(np.array(s_int))\n",
    "    \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        if w.lower() in word_vocab:\n",
    "            s_int.append(word_vocab[w.lower()])\n",
    "        else:\n",
    "            s_int.append(word_vocab['-OOV-'])\n",
    "    \n",
    "    test_sentences_X.append(np.array(s_int))\n",
    " \n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for t in s:\n",
    "        s_int.append(tag_vocab[t])\n",
    "    train_tags_y.append(np.array(s_int))\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for t in s:\n",
    "        s_int.append(tag_vocab[t])\n",
    "    test_tags_y.append(np.array(s_int))\n",
    " \n",
    " \n",
    "\n",
    "train_sentences_X = (np.asarray(train_sentences_X))\n",
    "test_sentences_X = (np.asarray(test_sentences_X))\n",
    "train_tags_y = (np.asarray(train_tags_y))\n",
    "test_tags_y = (np.asarray(test_tags_y))\n",
    "print (train_sentences_X[0])\n",
    "print (train_tags_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for element in train_sentences:\n",
    "    lengths.append(len(element))\n",
    "MAX_LENGTH = max(lengths)\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6193 4079  760 7038 1727 3884 6193 1727 9490 3352 5408  887 3884 7053\n",
      "  687 8522 4312 4465 8525 7856 4917 5102 1956 1819 3884 9561 4912 2499\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[ 4932  9714     1  5646  3884  3593  5980     2   821     1     1  9473\n",
      "   214  3884  2444  8497   871  1826  8914  7053   240  5980  3337 10103\n",
      "  9490  7831  5931  3364  7053  9489  3276  5333  3719  3884     1  7188\n",
      "   214  2499     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "[25 39 21  2 39 38 25 39  2 39  4 14 38 32 34 21 31 39 41 32 41  2 25 41\n",
      " 38 41  4 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[ 5  5  5  5 38 25 39  2  5 41 42  5  5 38 18 17 35 39 35 32 39 41 39 35\n",
      "  2  5  5  5 32 16 41 14 39 38  5  5  5 37  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#pad sequences with 0s until length = MAX_LENGTH\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 271)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 271, 24)           243288    \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 271, 128)          45568     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 271, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 271, 256)          263168    \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 271, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 271, 128)          164352    \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 271, 47)           6063      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 722,439\n",
      "Trainable params: 722,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "\n",
    "def POS_LSTM():\n",
    "    \n",
    "    inputs = Input(shape = (MAX_LENGTH, ))\n",
    "    X = Embedding(len(word_vocab), 24)(inputs)  \n",
    "    X = Bidirectional(LSTM(64, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dropout(0.8))(X)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dropout(0.8))(X)\n",
    "    X = Bidirectional(LSTM(64, return_sequences=True))(X)\n",
    "    X = TimeDistributed(Dense(len(tag_vocab)))(X)\n",
    "    outputs = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    " \n",
    "model = POS_LSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "(3131, 271, 47)\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.asarray(cat_sequences)\n",
    "\n",
    "print (len(tag_vocab))\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag_vocab))\n",
    "print (cat_train_tags_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/5\n",
      "2504/2504 [==============================] - 84s 34ms/step - loss: 1.6103 - acc: 0.8625 - val_loss: 0.6161 - val_acc: 0.9064\n",
      "Epoch 2/5\n",
      "2504/2504 [==============================] - 77s 31ms/step - loss: 0.5210 - acc: 0.9050 - val_loss: 0.3645 - val_acc: 0.9064\n",
      "Epoch 3/5\n",
      "2504/2504 [==============================] - 73s 29ms/step - loss: 0.3449 - acc: 0.9072 - val_loss: 0.3230 - val_acc: 0.9110\n",
      "Epoch 4/5\n",
      "2504/2504 [==============================] - 75s 30ms/step - loss: 0.3076 - acc: 0.9147 - val_loss: 0.3034 - val_acc: 0.9150\n",
      "Epoch 5/5\n",
      "2504/2504 [==============================] - 78s 31ms/step - loss: 0.2951 - acc: 0.9163 - val_loss: 0.2940 - val_acc: 0.9162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b3f210a58>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag_vocab)), batch_size=128, epochs= 5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 12s 15ms/step\n",
      "acc: 91.49642094189484\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag_vocab)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
